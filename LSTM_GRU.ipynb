{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7606b8",
      "metadata": {
        "id": "ef7606b8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99a6956",
      "metadata": {
        "scrolled": true,
        "id": "a99a6956",
        "outputId": "69ed163c-5078-4021-cd6e-2767a26338c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ProductID</th>\n",
              "      <th>ReviewID</th>\n",
              "      <th>ReviewTitle</th>\n",
              "      <th>ReviewTime</th>\n",
              "      <th>Verified</th>\n",
              "      <th>ReviewContent</th>\n",
              "      <th>ReviewRating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>These are hands down the best quality bands fo...</td>\n",
              "      <td>2016-01-16</td>\n",
              "      <td>False</td>\n",
              "      <td>These are hands down the best quality bands f...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>High Quality Bands</td>\n",
              "      <td>2016-01-22</td>\n",
              "      <td>False</td>\n",
              "      <td>I just got this set yesterday as well as a se...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>2015-12-27</td>\n",
              "      <td>False</td>\n",
              "      <td>My husband uses these and finds them to be go...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>The resistance is great. I would agree that th...</td>\n",
              "      <td>2016-01-13</td>\n",
              "      <td>False</td>\n",
              "      <td>I got these for Christmas and have been using...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Good quality product</td>\n",
              "      <td>2016-01-20</td>\n",
              "      <td>False</td>\n",
              "      <td>Haven\\t had it long enough to use all of the ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ProductID  ReviewID                                        ReviewTitle  \\\n",
              "0          1         1  These are hands down the best quality bands fo...   \n",
              "1          1         2                                 High Quality Bands   \n",
              "2          1         3                                         Five Stars   \n",
              "3          1         4  The resistance is great. I would agree that th...   \n",
              "4          1         5                               Good quality product   \n",
              "\n",
              "   ReviewTime  Verified                                      ReviewContent  \\\n",
              "0  2016-01-16     False   These are hands down the best quality bands f...   \n",
              "1  2016-01-22     False   I just got this set yesterday as well as a se...   \n",
              "2  2015-12-27     False   My husband uses these and finds them to be go...   \n",
              "3  2016-01-13     False   I got these for Christmas and have been using...   \n",
              "4  2016-01-20     False   Haven\\t had it long enough to use all of the ...   \n",
              "\n",
              "   ReviewRating  \n",
              "0           5.0  \n",
              "1           5.0  \n",
              "2           5.0  \n",
              "3           4.0  \n",
              "4           5.0  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('Amazon_Comments.csv', sep='^', header=None)\n",
        "df.columns = ['ProductID', 'ReviewID', 'ReviewTitle', 'ReviewTime', 'Verified', 'ReviewContent', 'ReviewRating']\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e3f5f9",
      "metadata": {
        "id": "94e3f5f9",
        "outputId": "28c38f07-5a0e-45e3-dec6-944f9adff91d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(275, 1763)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Splitting the reviews into negative and positive subsets\n",
        "negative_reviews = df[df['ReviewRating'].isin([1.0, 2.0, 3.0])]['ReviewContent'].dropna().tolist()\n",
        "positive_reviews = df[df['ReviewRating'].isin([4.0, 5.0])]['ReviewContent'].dropna().tolist()\n",
        "\n",
        "len(negative_reviews), len(positive_reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919cbb7c",
      "metadata": {
        "id": "919cbb7c"
      },
      "source": [
        "We have:\n",
        "\n",
        "275 negative reviews\n",
        "1763 positive reviews\n",
        "Next, we'll tokenize the reviews. We'll transform the text data into sequences of tokens. For this, we'll use the Tokenizer from Keras, which helps in converting text data into numerical format suitable for modeling.\n",
        "\n",
        "Let's proceed with the tokenization.​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5631fe52",
      "metadata": {
        "id": "5631fe52",
        "outputId": "c086738e-ba84-4f1c-cb1f-bdb53b89a2d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\agash\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\agash\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3999"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Downloading punkt tokenizer and stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Tokenization\n",
        "negative_tokens = [word_tokenize(review.lower()) for review in negative_reviews]\n",
        "positive_tokens = [word_tokenize(review.lower()) for review in positive_reviews]\n",
        "\n",
        "# Optional: Removing Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "negative_tokens = [[word for word in review if word not in stop_words] for review in negative_tokens]\n",
        "positive_tokens = [[word for word in review if word not in stop_words] for review in positive_tokens]\n",
        "\n",
        "# Optional: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "negative_tokens = [[stemmer.stem(word) for word in review] for review in negative_tokens]\n",
        "positive_tokens = [[stemmer.stem(word) for word in review] for review in positive_tokens]\n",
        "\n",
        "# Building a vocabulary\n",
        "vocab = set(word for review in negative_tokens + positive_tokens for word in review)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11de2f85",
      "metadata": {
        "id": "11de2f85"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694b8759",
      "metadata": {
        "id": "694b8759",
        "outputId": "b5b274ef-092a-44f4-da9d-40978371e242"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([['tore',\n",
              "   'littl',\n",
              "   'year',\n",
              "   ',',\n",
              "   'i\\\\m',\n",
              "   'strong',\n",
              "   '...',\n",
              "   'poor',\n",
              "   'qualiti',\n",
              "   ','],\n",
              "  ['littl',\n",
              "   'year',\n",
              "   ',',\n",
              "   'i\\\\m',\n",
              "   'strong',\n",
              "   '...',\n",
              "   'poor',\n",
              "   'qualiti',\n",
              "   ',',\n",
              "   'also'],\n",
              "  ['year',\n",
              "   ',',\n",
              "   'i\\\\m',\n",
              "   'strong',\n",
              "   '...',\n",
              "   'poor',\n",
              "   'qualiti',\n",
              "   ',',\n",
              "   'also',\n",
              "   'one'],\n",
              "  [',',\n",
              "   'i\\\\m',\n",
              "   'strong',\n",
              "   '...',\n",
              "   'poor',\n",
              "   'qualiti',\n",
              "   ',',\n",
              "   'also',\n",
              "   'one',\n",
              "   'handl'],\n",
              "  ['i\\\\m',\n",
              "   'strong',\n",
              "   '...',\n",
              "   'poor',\n",
              "   'qualiti',\n",
              "   ',',\n",
              "   'also',\n",
              "   'one',\n",
              "   'handl',\n",
              "   'got']],\n",
              " ['also', 'one', 'handl', 'got', 'bent'])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create sequences and targets\n",
        "\n",
        "sequence_length = 10  # We'll use the last N words...\n",
        "step = 1  # ...and sample a new sequence every M words\n",
        "\n",
        "def create_sequences(tokens):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(0, len(tokens) - sequence_length, step):\n",
        "        sequences.append(tokens[i:i+sequence_length])\n",
        "        targets.append(tokens[i+sequence_length])\n",
        "    return sequences, targets\n",
        "\n",
        "# Flattening the token lists and creating sequences and targets\n",
        "all_negative_tokens = [token for review in negative_tokens for token in review]\n",
        "all_positive_tokens = [token for review in positive_tokens for token in review]\n",
        "\n",
        "negative_sequences, negative_targets = create_sequences(all_negative_tokens)\n",
        "positive_sequences, positive_targets = create_sequences(all_positive_tokens)\n",
        "\n",
        "# Checking the first few sequences and targets\n",
        "negative_sequences[:5], negative_targets[:5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54285d0",
      "metadata": {
        "id": "f54285d0"
      },
      "source": [
        "We've created sequences of 10 tokens each, with the target being the next token in the review. For example, given the sequence ['They', 'tore', 'up', 'after', 'a', 'little', 'over', 'a', 'year,', 'and'], the target is ['I\\\\m'].\n",
        "\n",
        "Next, we need to convert these tokens into numerical values (token IDs) so they can be fed into the neural network. We'll also split the data into training and validation sets to evaluate the performance of our models.​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d6d579",
      "metadata": {
        "id": "86d6d579",
        "outputId": "ae11e73e-d35c-4ddf-9263-7d6eb4551c9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5656, 1414, 24953, 6239)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert tokens to their respective IDs\n",
        "def tokens_to_ids(sequences, targets, vocab):\n",
        "    sequence_ids = [[vocab[word] if word in vocab else vocab[\"<OOV>\"] for word in sequence] for sequence in sequences]\n",
        "    target_ids = [vocab[word] if word in vocab else vocab[\"<OOV>\"] for word in targets]\n",
        "    return sequence_ids, target_ids\n",
        "\n",
        "# Create a vocabulary dictionary\n",
        "vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
        "vocab_dict[\"<OOV>\"] = len(vocab_dict)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "negative_sequence_ids, negative_target_ids = tokens_to_ids(negative_sequences, negative_targets, vocab_dict)\n",
        "positive_sequence_ids, positive_target_ids = tokens_to_ids(positive_sequences, positive_targets, vocab_dict)\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "neg_X_train, neg_X_val, neg_y_train, neg_y_val = train_test_split(negative_sequence_ids, negative_target_ids, test_size=0.2, random_state=42)\n",
        "pos_X_train, pos_X_val, pos_y_train, pos_y_val = train_test_split(positive_sequence_ids, positive_target_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "len(neg_X_train), len(neg_X_val), len(pos_X_train), len(pos_X_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8922cab",
      "metadata": {
        "id": "f8922cab"
      },
      "source": [
        "Successfully converted the tokens into their respective IDs and split the data into training and validation sets. Here's the breakdown:\n",
        "\n",
        "Negative reviews:\n",
        "Training: 8,989 samples\n",
        "Validation: 2,248 samples\n",
        "Positive reviews:\n",
        "Training: 37,972 samples\n",
        "Validation: 9,494 samples\n",
        "Building  LSTM model for text generation. After training the LSTM model, we'll move on to the GRU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243130a8",
      "metadata": {
        "id": "243130a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e86687bb",
      "metadata": {
        "id": "e86687bb"
      },
      "source": [
        "## LSTM Model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b10c1db",
      "metadata": {
        "id": "5b10c1db"
      },
      "source": [
        "Data Preparation:\n",
        "Converting sequences and targets into tensors suitable for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1474f3e",
      "metadata": {
        "id": "e1474f3e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert sequences and targets to torch tensors\n",
        "train_data = TensorDataset(torch.tensor(neg_X_train + pos_X_train), torch.tensor(neg_y_train + pos_y_train))\n",
        "val_data = TensorDataset(torch.tensor(neg_X_val + pos_X_val), torch.tensor(neg_y_val + pos_y_val))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c012051",
      "metadata": {
        "id": "1c012051"
      },
      "source": [
        "Installing torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27aeade8",
      "metadata": {
        "id": "27aeade8"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8225923",
      "metadata": {
        "id": "d8225923"
      },
      "source": [
        "Model Definition:\n",
        "Define the LSTM model using PyTorch.\n",
        "python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50086d0",
      "metadata": {
        "id": "f50086d0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        predictions = self.fc(lstm_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "output_dim = vocab_size\n",
        "\n",
        "# Create the LSTM model instance\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35eb1a7",
      "metadata": {
        "id": "d35eb1a7"
      },
      "source": [
        "Training:\n",
        "Train the LSTM model on  sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74959063",
      "metadata": {
        "id": "74959063",
        "outputId": "c722fd90-c307-4ed3-82af-1aae407a5afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 6.3629\n",
            "Epoch [2/10], Loss: 5.9780\n",
            "Epoch [3/10], Loss: 5.9912\n",
            "Epoch [4/10], Loss: 4.7349\n",
            "Epoch [5/10], Loss: 5.1147\n",
            "Epoch [6/10], Loss: 4.1417\n",
            "Epoch [7/10], Loss: 3.2236\n",
            "Epoch [8/10], Loss: 2.4758\n",
            "Epoch [9/10], Loss: 1.3389\n",
            "Epoch [10/10], Loss: 0.3507\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_seq, batch_target in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_seq)\n",
        "        loss = criterion(outputs, batch_target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c2e6a5",
      "metadata": {
        "id": "e1c2e6a5"
      },
      "source": [
        "Evaluation:\n",
        "Evaluating the model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749e092c",
      "metadata": {
        "id": "749e092c",
        "outputId": "1fa59dec-bbf7-4b06-dcc5-1946322bd1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 9.5330\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    for batch_seq, batch_target in val_loader:\n",
        "        outputs = model(batch_seq)\n",
        "        loss = criterion(outputs, batch_target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10919c37",
      "metadata": {
        "id": "10919c37"
      },
      "source": [
        "Text Generation:\n",
        "To generate text, seed the model with a starting sequence, predict the next word, append it to the sequence, and repeat."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c16d78",
      "metadata": {
        "id": "44c16d78"
      },
      "source": [
        "We'll structure the text generation in such a way that:\n",
        "\n",
        "\n",
        "For a negative review, we seed the model with a sequence from a negative review.\n",
        "\n",
        "For a positive review, we seed the model with a sequence from a positive review.\n",
        "\n",
        "The idea is that by seeding with a particular sentiment (positive or negative), the model is more likely to continue generating text in that sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194fba6c",
      "metadata": {
        "id": "194fba6c",
        "outputId": "943149c2-e070-4100-9247-f214ba8306c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of predictions[0]: torch.Size([8833])\n",
            "Dimension error encountered. Printing shape for diagnosis.\n",
            "torch.Size([8833])\n",
            "Generated Negative Review: None\n"
          ]
        }
      ],
      "source": [
        "def generate_review_text(model, seed_sequence, generation_length=50):\n",
        "    generated_text = seed_sequence.copy()\n",
        "\n",
        "    for _ in range(generation_length):\n",
        "        # Convert seed sequence to tensor\n",
        "        seed_tensor = torch.tensor(seed_sequence).unsqueeze(0)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(seed_tensor)\n",
        "\n",
        "        print(f\"Shape of predictions[0]: {predictions[0].shape}\")  # Diagnostic print\n",
        "\n",
        "        # Try catching the dimension error and print the shape for diagnosis\n",
        "        try:\n",
        "            predicted_token_id = torch.argmax(predictions[0], dim=1).item()\n",
        "        except IndexError:\n",
        "            print(\"Dimension error encountered. Printing shape for diagnosis.\")\n",
        "            print(predictions[0].shape)\n",
        "            return\n",
        "\n",
        "        # Append the predicted token ID to the generated text and seed sequence\n",
        "        generated_text.append(predicted_token_id)\n",
        "        seed_sequence = generated_text[-sequence_length:]\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    generated_review = [vocab_dict[token_id] for token_id in generated_text]\n",
        "\n",
        "    return ' '.join(generated_review)\n",
        "\n",
        "# Example Usage:\n",
        "negative_seed = neg_X_train[0]  # or any other negative sequence\n",
        "generated_negative_review = generate_review_text(model, negative_seed)\n",
        "print(\"Generated Negative Review:\", generated_negative_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c0cef6",
      "metadata": {
        "id": "42c0cef6",
        "outputId": "c7837498-501b-4511-ccbe-936bf5da91cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Negative Review: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "Generated Positive Review: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n"
          ]
        }
      ],
      "source": [
        "def generate_review_text(model, seed_sequence, generation_length=50):\n",
        "    generated_text = seed_sequence.copy()\n",
        "\n",
        "    for _ in range(generation_length):\n",
        "        # Convert seed sequence to tensor\n",
        "        seed_tensor = torch.tensor(seed_sequence).unsqueeze(0)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(seed_tensor)\n",
        "\n",
        "        # Use dim=0 since predictions[0] is a 1D tensor\n",
        "        predicted_token_id = torch.argmax(predictions[0], dim=0).item()\n",
        "\n",
        "        # Append the predicted token ID to the generated text and seed sequence\n",
        "        generated_text.append(predicted_token_id)\n",
        "        seed_sequence = generated_text[-sequence_length:]\n",
        "\n",
        "    # Convert token IDs back to words, using '<UNK>' for unknown tokens\n",
        "    generated_review = [vocab_dict.get(token_id, '<UNK>') for token_id in generated_text]\n",
        "\n",
        "    return ' '.join(generated_review)\n",
        "\n",
        "# Example Usage:\n",
        "negative_seed = neg_X_train[0]  # or any other negative sequence\n",
        "generated_negative_review = generate_review_text(model, negative_seed)\n",
        "print(\"Generated Negative Review:\", generated_negative_review)\n",
        "\n",
        "positive_seed = pos_X_train[0]  # or any other positive sequence\n",
        "generated_positive_review = generate_review_text(model, positive_seed)\n",
        "print(\"Generated Positive Review:\", generated_positive_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cbce83",
      "metadata": {
        "id": "08cbce83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c1046c63",
      "metadata": {
        "id": "c1046c63"
      },
      "source": [
        "## GRU MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b55e83",
      "metadata": {
        "id": "f8b55e83"
      },
      "source": [
        "1. Model Definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d844aab",
      "metadata": {
        "id": "8d844aab"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        predictions = self.fc(gru_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "output_dim = vocab_size\n",
        "\n",
        "# Create the GRU model instance\n",
        "model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96ae445",
      "metadata": {
        "id": "f96ae445"
      },
      "source": [
        "2. Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d7b026",
      "metadata": {
        "id": "41d7b026",
        "outputId": "bb7115a6-28bd-4519-b0dd-98f231a6066a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 5.5932\n",
            "Epoch [2/10], Loss: 5.4356\n",
            "Epoch [3/10], Loss: 4.3285\n",
            "Epoch [4/10], Loss: 2.9671\n",
            "Epoch [5/10], Loss: 1.1337\n",
            "Epoch [6/10], Loss: 0.1608\n",
            "Epoch [7/10], Loss: 0.0367\n",
            "Epoch [8/10], Loss: 0.0103\n",
            "Epoch [9/10], Loss: 0.0095\n",
            "Epoch [10/10], Loss: 2.3334\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_seq, batch_target in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_seq)\n",
        "        loss = criterion(outputs, batch_target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168ea5a1",
      "metadata": {
        "id": "168ea5a1"
      },
      "source": [
        "3. Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd51a1cd",
      "metadata": {
        "id": "bd51a1cd",
        "outputId": "cb2c5703-4185-4eb0-f39c-0b2b4ffa49ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 8.9342\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    for batch_seq, batch_target in val_loader:\n",
        "        outputs = model(batch_seq)\n",
        "        loss = criterion(outputs, batch_target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb0a9bf",
      "metadata": {
        "id": "acb0a9bf"
      },
      "source": [
        "4. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e87cb5",
      "metadata": {
        "id": "97e87cb5",
        "outputId": "39b48228-3aaf-437e-8100-5c7de1308a3c"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "960",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Seed the model with a sequence from a negative review\u001b[39;00m\n\u001b[0;32m     26\u001b[0m negative_seed_gru \u001b[38;5;241m=\u001b[39m neg_X_train[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# or any other negative sequence\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m generated_negative_review_gru \u001b[38;5;241m=\u001b[39m generate_review_text_gru(model, negative_seed_gru)  \u001b[38;5;66;03m# Using 'model' instead of 'gru_model'\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Negative Review (GRU):\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_negative_review_gru)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Seed the model with a sequence from a positive review\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[47], line 20\u001b[0m, in \u001b[0;36mgenerate_review_text_gru\u001b[1;34m(model, seed_sequence, generation_length)\u001b[0m\n\u001b[0;32m     17\u001b[0m     seed_sequence \u001b[38;5;241m=\u001b[39m generated_text[\u001b[38;5;241m-\u001b[39msequence_length:]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert token IDs back to words\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m generated_review \u001b[38;5;241m=\u001b[39m [vocab_dict[token_id] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m generated_text]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_review)\n",
            "Cell \u001b[1;32mIn[47], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     seed_sequence \u001b[38;5;241m=\u001b[39m generated_text[\u001b[38;5;241m-\u001b[39msequence_length:]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert token IDs back to words\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m generated_review \u001b[38;5;241m=\u001b[39m [vocab_dict[token_id] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m generated_text]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_review)\n",
            "\u001b[1;31mKeyError\u001b[0m: 960"
          ]
        }
      ],
      "source": [
        "def generate_review_text_gru(model, seed_sequence, generation_length=50):\n",
        "    generated_text = seed_sequence.copy()\n",
        "\n",
        "    for _ in range(generation_length):\n",
        "        # Convert seed sequence to tensor\n",
        "        seed_tensor = torch.tensor(seed_sequence).unsqueeze(0)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(seed_tensor)\n",
        "\n",
        "        # Get the token ID with the highest prediction probability\n",
        "        predicted_token_id = torch.argmax(predictions[0], dim=0).item()\n",
        "\n",
        "        # Append the predicted token ID to the generated text and seed sequence\n",
        "        generated_text.append(predicted_token_id)\n",
        "        seed_sequence = generated_text[-sequence_length:]\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    generated_review = [vocab_dict[token_id] for token_id in generated_text]\n",
        "\n",
        "    return ' '.join(generated_review)\n",
        "\n",
        "\n",
        "# Seed the model with a sequence from a negative review\n",
        "negative_seed_gru = neg_X_train[0]  # or any other negative sequence\n",
        "generated_negative_review_gru = generate_review_text_gru(model, negative_seed_gru)  # Using 'model' instead of 'gru_model'\n",
        "print(\"Generated Negative Review (GRU):\", generated_negative_review_gru)\n",
        "\n",
        "# Seed the model with a sequence from a positive review\n",
        "positive_seed_gru = pos_X_train[0]  # or any other positive sequence\n",
        "generated_positive_review_gru = generate_review_text_gru(model, positive_seed_gru)  # Using 'model' instead of 'gru_model'\n",
        "print(\"Generated Positive Review (GRU):\", generated_positive_review_gru)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef76f9f",
      "metadata": {
        "id": "1ef76f9f",
        "outputId": "199b4e9a-bb0b-467a-d5d8-d5681324a060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Negative Review (v2): <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "Generated Positive Review (v2): <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n"
          ]
        }
      ],
      "source": [
        "def generate_review_text_v2(model, seed_sequence, generation_length=50):\n",
        "    generated_text = list(seed_sequence)  # Using list() to ensure a copy is made\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    for _ in range(generation_length):\n",
        "        # Convert seed sequence to tensor\n",
        "        seed_tensor = torch.tensor(seed_sequence, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(seed_tensor)\n",
        "\n",
        "        # Use dim=0 since predictions[0] is a 1D tensor\n",
        "        predicted_token_id = torch.argmax(predictions[0], dim=0).item()\n",
        "\n",
        "        # Append the predicted token ID to the generated text\n",
        "        generated_text.append(predicted_token_id)\n",
        "\n",
        "        # Update the seed sequence for the next iteration\n",
        "        seed_sequence = generated_text[-sequence_length:]\n",
        "\n",
        "    # Convert token IDs back to words, using '<UNK>' for unknown tokens\n",
        "    generated_review = [vocab_dict.get(token_id, '<UNK>') for token_id in generated_text]\n",
        "\n",
        "    return ' '.join(generated_review)\n",
        "\n",
        "# Example Usage:\n",
        "negative_seed = neg_X_train[0]  # or any other negative sequence\n",
        "generated_negative_review = generate_review_text_v2(model, negative_seed)\n",
        "print(\"Generated Negative Review (v2):\", generated_negative_review)\n",
        "\n",
        "positive_seed = pos_X_train[0]  # or any other positive sequence\n",
        "generated_positive_review = generate_review_text_v2(model, positive_seed)\n",
        "print(\"Generated Positive Review (v2):\", generated_positive_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591492d5",
      "metadata": {
        "id": "591492d5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}